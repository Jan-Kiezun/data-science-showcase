{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.9, epsilon=1e-6):\n",
    "    # Initialize value function\n",
    "    nS = env.nrow * env.ncol\n",
    "    nA = 4\n",
    "    V = np.zeros(nS)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(nS):\n",
    "            max_q = -np.inf\n",
    "            for a in range(nA):\n",
    "                q = 0\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    q += prob * (reward + gamma * V[next_state])\n",
    "                max_q = max(max_q, q)\n",
    "            \n",
    "            delta = max(delta, np.abs(max_q - V[s]))\n",
    "            V[s] = max_q\n",
    "        \n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    # Compute optimal policy\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    for s in range(nS):\n",
    "        max_q = -np.inf\n",
    "        best_action = 0\n",
    "        for a in range(nA):\n",
    "            q = 0\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q += prob * (reward + gamma * V[next_state])\n",
    "            if q > max_q:\n",
    "                max_q = q\n",
    "                best_action = a\n",
    "        \n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janki\\Dev\\School\\Y2\\IO\\lab9\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "9\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Create the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "\n",
    "# Run value iteration\n",
    "optimal_policy = value_iteration(env)\n",
    "print(optimal_policy)\n",
    "\n",
    "# Play one episode using the learned policy\n",
    "state, _ = env.reset()\n",
    "print(state)\n",
    "done = False\n",
    "while not done:\n",
    "    action = optimal_policy[state]\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    print(state)\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States =  16\n",
      "Actions =  4\n",
      "Percent of episodes finished successfully: 0.451\n",
      "Percent of episodes finished successfully (last 100 episodes): 0.68\n",
      "Average number of steps: 7.91\n",
      "Average number of steps (last 100 episodes): 6.32\n",
      "[[0.73509189 0.77378094 0.77378094 0.73509189]\n",
      " [0.73509189 0.         0.81450625 0.77378094]\n",
      " [0.77378094 0.857375   0.77358935 0.81450625]\n",
      " [0.81449089 0.         0.69508966 0.        ]\n",
      " [0.77378094 0.81450625 0.         0.73509189]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9025     0.         0.81450616]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.81450625 0.         0.857375   0.77378094]\n",
      " [0.81450625 0.9025     0.9025     0.        ]\n",
      " [0.857375   0.95       0.         0.857375  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9025     0.95       0.857375  ]\n",
      " [0.9025     0.95       1.         0.9025    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Create the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.reset(seed=0)\n",
    "\n",
    "# Q-learning\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "print( \"States = \", number_of_states)\n",
    "print( \"Actions = \", number_of_actions)\n",
    "\n",
    "num_episodes = 1000\n",
    "steps_total = []\n",
    "rewards_total = []\n",
    "egreedy_total = []\n",
    "\n",
    "# PARAMS \n",
    "# Discount on reward\n",
    "gamma = 0.95\n",
    "# Factor to balance the ratio of action taken based on past experience to current situtation\n",
    "learning_rate = 0.9\n",
    "\n",
    "egreedy = 0.7\n",
    "egreedy_final = 0.1\n",
    "egreedy_decay = 0.999\n",
    "\n",
    "Q = np.zeros([number_of_states, number_of_actions])\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        \n",
    "        # Act greedy sometimes to allow exploration\n",
    "        if np.random.rand(1) < egreedy:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "        \n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        # Update Q-Table with new knowledge\n",
    "        Q[state, action] = Q[state, action] + learning_rate * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            egreedy_total.append(egreedy)\n",
    "            # print(\"Episode finished after %i steps\" % step)\n",
    "            break\n",
    "    \n",
    "    # Reduce chance of random action as we train the model.\n",
    "    egreedy *= egreedy_decay\n",
    "    egreedy = max(egreedy_final, egreedy)\n",
    "\n",
    "print(\"Percent of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes))\n",
    "print(\"Percent of episodes finished successfully (last 100 episodes): {0}\".format(sum(rewards_total[-100:])/100))\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average number of steps (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "state, _ = env.reset(seed=0)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(Q[state, :])\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def deep_q_learning(env, num_episodes=1000, gamma=0.99, epsilon=1.0, epsilon_decay=0.99, epsilon_min=0.01, batch_size=32):\n",
    "    input_dim = env.observation_space.n\n",
    "    output_dim = env.action_space.n\n",
    "    \n",
    "    # Build the Deep Q-network\n",
    "    model = QNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Initialize replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy exploration strategy\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model(torch.FloatTensor(np.eye(input_dim)[state]))\n",
    "                action = torch.argmax(q_values).item()\n",
    "            \n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            # Store the transition in replay memory\n",
    "            replay_memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            # Sample a random minibatch from replay memory\n",
    "            if len(replay_memory) >= batch_size:\n",
    "                minibatch = np.random.choice(len(replay_memory), batch_size, replace=False)\n",
    "                \n",
    "                states_mb = torch.FloatTensor(np.eye(input_dim)[np.array([replay_memory[idx][0] for idx in minibatch])])\n",
    "                actions_mb = torch.LongTensor(np.array([replay_memory[idx][1] for idx in minibatch]))\n",
    "                rewards_mb = torch.FloatTensor(np.array([replay_memory[idx][2] for idx in minibatch]))\n",
    "                next_states_mb = torch.FloatTensor(np.eye(input_dim)[np.array([replay_memory[idx][3] for idx in minibatch])])\n",
    "                dones_mb = torch.FloatTensor(np.array([replay_memory[idx][4] for idx in minibatch]))\n",
    "                \n",
    "                # Calculate target Q-values using the Bellman equation\n",
    "                Q_targets = model(states_mb).gather(1, actions_mb.unsqueeze(1)).squeeze(1)\n",
    "                Q_targets_next = model(next_states_mb).max(1)[0].detach()\n",
    "                Q_targets = rewards_mb + gamma * (1 - dones_mb) * Q_targets_next\n",
    "                \n",
    "                # Update the Q-network\n",
    "                loss = F.mse_loss(Q_targets.unsqueeze(1), model(states_mb).gather(1, actions_mb.unsqueeze(1)))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Decay epsilon\n",
    "            epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "    \n",
    "    # Extract optimal policy from the trained Q-network\n",
    "    policy = np.argmax(model(torch.FloatTensor(np.eye(input_dim))).detach().numpy(), axis=1)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 1 1 3 2 2 2 1 2 2 3 2 0]\n"
     ]
    }
   ],
   "source": [
    "# Create the FrozenLake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.reset(seed=0)\n",
    "# params\n",
    "num_episodes = 200\n",
    "batch_size = 8\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.97\n",
    "epsilon_min = 0.01\n",
    "\n",
    "optimal_policy = deep_q_learning(env, num_episodes, gamma, epsilon, epsilon_decay, epsilon_min, batch_size)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "state, _ = env.reset(seed=0)\n",
    "done = False\n",
    "while not done:\n",
    "    action = optimal_policy[state]\n",
    "    state, reward, done, *_ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
